# UAR-ACSSNet: å•é€šé“ EEG å»ä¼ªå½±/é‡å»ºç³»ç»Ÿ

> **Unified Artifact Removal with Axis-Conditioned Selective Scan Network**
> 
> åŸºäº PyTorch çš„ç«¯åˆ°ç«¯å•é€šé“ EEG ä¿¡å·å»å™ªä¸é‡å»ºæ¡†æ¶

---

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [æ¨¡å‹æ¶æ„](#æ¨¡å‹æ¶æ„)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [æ•°æ®é›†æ ¼å¼](#æ•°æ®é›†æ ¼å¼)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
- [è®­ç»ƒä¸éªŒè¯](#è®­ç»ƒä¸éªŒè¯)
- [æ¨ç†ä¸éƒ¨ç½²](#æ¨ç†ä¸éƒ¨ç½²)
- [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)
- [å¯å¤ç°æ€§](#å¯å¤ç°æ€§)

---

## æ¦‚è¿°

UAR-ACSSNet æ˜¯ä¸€ä¸ªä¸“ä¸ºå•é€šé“ EEG ä¿¡å·å»ä¼ªå½±è®¾è®¡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å®ƒç»“åˆäº†æ—¶åŸŸå’Œæ—¶é¢‘åŸŸä¿¡æ¯ï¼Œé€šè¿‡è½´å‘æ¡ä»¶é€‰æ‹©æ€§æ‰«æï¼ˆAxis-Conditioned Selective Scanï¼‰æœºåˆ¶ï¼Œå®ç°é«˜ä¿çœŸçš„ä¿¡å·é‡å»ºå’Œè‡ªé€‚åº”ç½®ä¿¡åº¦ä¼°è®¡ã€‚

**æ ¸å¿ƒåˆ›æ–°ç‚¹ï¼š**
1. **æ—¶åŸŸ-æ—¶é¢‘åŒåˆ†æ”¯æ¶æ„**ï¼š1D U-Net è´Ÿè´£æ—¶åŸŸé‡å»ºï¼Œ2D æ—¶é¢‘ç¼–ç å™¨æå–é¢‘åŸŸç‰¹å¾
2. **ACSSBlockï¼ˆè½´å‘æ¡ä»¶é€‰æ‹©æ€§æ‰«æå—ï¼‰**ï¼šå¯æ›¿ä»£çš„ Mamba-like æ‰«ææœºåˆ¶ï¼Œæ²¿æ—¶é—´å’Œé¢‘ç‡è½´èšåˆä¿¡æ¯
3. **FiLM è·¨åŸŸèåˆ**ï¼šå°†æ—¶é¢‘ç‰¹å¾é€šè¿‡ Feature-wise Linear Modulation è°ƒåˆ¶æ—¶åŸŸè§£ç å™¨
4. **è‡ªé€‚åº”ç½®ä¿¡å›¾**ï¼šè¾“å‡ºé€æ ·æœ¬ç‚¹çš„ç½®ä¿¡åº¦ï¼Œæ”¯æŒåŠ æƒæŸå¤±

---

## æ ¸å¿ƒç‰¹æ€§

- âœ… **ç«¯åˆ°ç«¯å¯è®­ç»ƒ**ï¼šä»åŸå§‹ EEG åˆ°å»å™ªä¿¡å·ï¼Œæ— éœ€æ‰‹å·¥ç‰¹å¾æå–
- âœ… **æ—¶é¢‘è”åˆå»ºæ¨¡**ï¼šSTFT å›ºå®šå‚æ•°ï¼ˆfs=500Hz, 1-100Hzï¼‰ï¼Œç¡®ä¿å¯å¤ç°
- âœ… **å¯æ›¿ä»£æ‰«ææœºåˆ¶**ï¼šACSSBlock å†…éƒ¨ä½¿ç”¨ç®€åŒ–çš„ depthwise conv æ¨¡æ‹Ÿæ‰«æï¼Œæœªæ¥å¯æ›¿æ¢ä¸ºçœŸå® Mamba/SSM
- âœ… **çµæ´»çš„æ•°æ®é›†æ”¯æŒ**ï¼šæ”¯æŒå˜é•¿ä¿¡å·ã€æ»‘çª—åˆ‡ç‰‡ã€Z-score å½’ä¸€åŒ–
- âœ… **å®Œå–„çš„æŸå¤±å‡½æ•°**ï¼šCharbonnier é‡å»ºæŸå¤± + ç½®ä¿¡å›¾æ­£åˆ™ï¼ˆTV + ç†µï¼‰+ ä¸€è‡´æ€§æŸå¤±æ¥å£
- âœ… **å®Œæ•´çš„è®­ç»ƒæ¡†æ¶**ï¼šåŒ…å« train/val/test æ•°æ®åŠ è½½ã€æ¢¯åº¦è£å‰ªã€å­¦ä¹ ç‡è°ƒåº¦ã€æ¨¡å‹ä¿å­˜

---

## æ¨¡å‹æ¶æ„

### æ•´ä½“ç»“æ„

```
è¾“å…¥ x_raw (B, 1, L)
    â”œâ”€ æ—¶åŸŸåˆ†æ”¯: 1D U-Net
    â”‚   â”œâ”€ Encoder (4 å±‚ä¸‹é‡‡æ ·)
    â”‚   â”œâ”€ Bottleneck (æ®‹å·®å—)
    â”‚   â””â”€ Decoder (4 å±‚ä¸Šé‡‡æ · + skip connections)
    â”‚       â””â”€ FiLM è°ƒåˆ¶ (ç”±æ—¶é¢‘ç‰¹å¾ç”Ÿæˆ Î±, Î²)
    â”‚
    â”œâ”€ æ—¶é¢‘åˆ†æ”¯: STFT + SpecEncoder2D + ACSSStack
    â”‚   â”œâ”€ STFT (å›ºå®šå‚æ•°: n_fft=512, hop=64, win=156)
    â”‚   â”‚   â””â”€ é€‰æ‹© 1-100 Hz bins â†’ (B, F_sel, T)
    â”‚   â”œâ”€ SpecEncoder2D: (B, F_sel, T) â†’ (B, C, T, F)
    â”‚   â””â”€ ACSSStack (K å±‚ ACSSBlock2D)
    â”‚       â””â”€ æ¯å±‚: Axis Summary â†’ Gate â†’ Selective Scan â†’ Mixture â†’ Residual
    â”‚
    â””â”€ èåˆ & è¾“å‡º
        â”œâ”€ FiLM Generator: ä»æ—¶é¢‘ç‰¹å¾ç”Ÿæˆè°ƒåˆ¶å‚æ•°
        â”œâ”€ é‡å»ºè¾“å‡º: y_hat (B, 1, L)
        â””â”€ ç½®ä¿¡å›¾: w (B, 1, L) âˆˆ [0, 1]
```

### ACSSBlock2D è¯¦è§£

**è¾“å…¥/è¾“å‡º**: `(B, C, T, F)`  
**åŒ…å«æ¨¡å—**:

1. **Axis Summaryï¼ˆè¯æ®æå–ï¼‰**
   ```
   é¢‘è½´æ‘˜è¦: (B,C,T,F) --pool F--> (B,2C,T)  [mean+std]
   æ—¶è½´æ‘˜è¦: (B,C,T,F) --pool T--> (B,2C,F)
   ```

2. **Axis-conditioned Gateï¼ˆè½´å‘è‡ªé€‚åº”é—¨æ§ï¼‰**
   ```
   s_f (B,2C,T) --MLP--> g_freq (B,1,T) âˆˆ [0,1]
   ```

3. **Selective Scan Mixture**
   ```
   U_freq = ScanFreq(X)  # æ²¿é¢‘ç‡è½´æ‰«æ
   U_time = ScanTime(X)  # æ²¿æ—¶é—´è½´æ‰«æ
   Y = g * U_freq + (1-g) * U_time
   ```
   
   **æ³¨**ï¼š`ScanFreq/ScanTime` å½“å‰ä½¿ç”¨ depthwise Conv1D æ¨¡æ‹Ÿï¼Œæ¥å£è®¾è®¡ä¾¿äºæ›¿æ¢ä¸ºçœŸå® Mamba/SSM

4. **Residual + Norm**
   ```
   out = X + Proj(Y)
   ```

### FiLM è°ƒåˆ¶æœºåˆ¶

ä»æ—¶é¢‘åˆ†æ”¯æå–çš„ç‰¹å¾ `X_tf (B, C, T, F)` ç»è¿‡é¢‘ç‡ç»´ pooling å¾—åˆ° `m(t) (B, C, T)`ï¼Œæ’å€¼åˆ°æ—¶åŸŸé•¿åº¦ `L` åç”Ÿæˆï¼š

```
Î±, Î² (B, C_dec, L)
H' = Î± âŠ™ H + Î²  # å¯¹ U-Net decoder çš„ç‰¹å¾è¿›è¡Œè°ƒåˆ¶
```

åº”ç”¨äº decoder çš„å‰ä¸¤å±‚ï¼ˆå¯¹åº” 2 ä¸ªæœ€é«˜åˆ†è¾¨ç‡å±‚ï¼‰ï¼Œå®ç°è·¨åŸŸä¿¡æ¯èåˆã€‚

---

## é¡¹ç›®ç»“æ„

```
cDDPM/V5/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ eeg_pair_dataset.py    # EEGPairDataset ç±»
â”‚   â”œâ”€â”€ build_loaders.py       # æ•°æ®åŠ è½½å™¨æ„å»ºå‡½æ•°
â”‚   â””â”€â”€ transforms.py          # æ•°æ®å¢å¼ºå˜æ¢
â”œâ”€â”€ signal_processing/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ stft_utils.py          # STFTProcessor (å›ºå®šå‚æ•° STFT)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ uar_acssnet.py         # å®Œæ•´æ¨¡å‹å®ç°
â”‚       â”œâ”€â”€ ResidualBlock1D, DownBlock1D, UpBlock1D
â”‚       â”œâ”€â”€ UNet1D (æ—¶åŸŸä¸»å¹²)
â”‚       â”œâ”€â”€ DepthwiseScan1D, ScanFreq, ScanTime (å¯æ›¿ä»£æ‰«æ)
â”‚       â”œâ”€â”€ ACSSBlock2D (æ ¸å¿ƒæ¨¡å—)
â”‚       â”œâ”€â”€ SpecEncoder2D (æ—¶é¢‘ç¼–ç å™¨)
â”‚       â”œâ”€â”€ FiLMGenerator1D (è·¨åŸŸèåˆ)
â”‚       â””â”€â”€ UAR_ACSSNet (å®Œæ•´æ¨¡å‹)
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ losses.py              # æŸå¤±å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ CharbonnierLoss, HuberLoss
â”‚   â”‚   â”œâ”€â”€ ConfidenceRegularization (TV + ç†µ)
â”‚   â”‚   â”œâ”€â”€ ConsistencyLoss (ä¸€è‡´æ€§æŸå¤±æ¥å£)
â”‚   â”‚   â””â”€â”€ compute_losses (æ€»æŸå¤±è®¡ç®—)
â”‚   â””â”€â”€ min_train.py           # è®­ç»ƒå…¥å£
â”‚       â”œâ”€â”€ train_one_epoch, validate
â”‚       â”œâ”€â”€ train (å®Œæ•´è®­ç»ƒå¾ªç¯)
â”‚       â””â”€â”€ main_minimal_example (éšæœºæ•°æ®æµ‹è¯•)
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ default.py             # é»˜è®¤é…ç½®
â”œâ”€â”€ inference_file.py          # ğŸ†• æ–‡ä»¶çº§æ¨ç†è„šæœ¬
â”œâ”€â”€ visualize_inference.py     # ğŸ†• æ¨ç†ç»“æœå¯è§†åŒ–
â”œâ”€â”€ test_inference.py          # ğŸ†• æ¨ç†åŠŸèƒ½æµ‹è¯•
â”œâ”€â”€ example_inference_api.py   # ğŸ†• Python API ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ main.py                    # è®­ç»ƒä¸»å…¥å£
â”œâ”€â”€ INFERENCE_README.md        # ğŸ†• æ¨ç†å®Œæ•´æ–‡æ¡£
â””â”€â”€ README.md                  # æœ¬æ–‡æ¡£
```

---

## æ•°æ®é›†æ ¼å¼

### ç›®å½•ç»“æ„ï¼ˆå›ºå®šï¼‰

```
Dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ raw/       # åŸå§‹å¸¦ä¼ªå½±ä¿¡å·
â”‚   â”‚   â”œâ”€â”€ 0001.npy
â”‚   â”‚   â”œâ”€â”€ 0002.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ clean/     # å¹²å‡€å‚è€ƒä¿¡å·
â”‚       â”œâ”€â”€ 0001.npy
â”‚       â”œâ”€â”€ 0002.npy
â”‚       â””â”€â”€ ...
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ clean/
â””â”€â”€ test/
    â”œâ”€â”€ raw/
    â””â”€â”€ clean/
```

### æ–‡ä»¶æ ¼å¼è¦æ±‚

- **æ ¼å¼**: `.npy` (NumPy array)
- **Shape**: `(L,)` æˆ– `(1, L)` ï¼ˆä»£ç ä¼šè‡ªåŠ¨ç»Ÿä¸€ä¸º `(1, L)`ï¼‰
- **Dtype**: æ¨è `float32`
- **é…å¯¹**: `raw/` å’Œ `clean/` ä¸‹çš„æ–‡ä»¶åå¿…é¡»ä¸€ä¸€å¯¹åº”

### æ•°æ®é›†ç‰¹æ€§æ”¯æŒ

- âœ… **å˜é•¿ä¿¡å·**: è‡ªåŠ¨é›¶å¡«å……åˆ° `segment_length`ï¼ˆè®°å½•åœ¨ `meta['is_padded']`ï¼‰
- âœ… **æ»‘çª—åˆ‡ç‰‡**: é€šè¿‡ `stride` å‚æ•°ç”Ÿæˆç¡®å®šæ€§åˆ‡ç‰‡ï¼ˆç”¨äº val/testï¼‰
- âœ… **éšæœºè£å‰ª**: train æ¨¡å¼æ”¯æŒéšæœºè£å‰ªï¼ˆè®¾ç½® `random_crop=True`ï¼‰
- âœ… **Z-score å½’ä¸€åŒ–**: é€æ ·æœ¬å½’ä¸€åŒ–ï¼ˆå¯é€‰ `"zscore_per_sample"` æˆ– `"none"`ï¼‰

---

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰
conda create -n eeg_denoise python=3.9
conda activate eeg_denoise

# å®‰è£…ä¾èµ–
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install numpy scipy
```

### 2. æµ‹è¯•æ¨¡å—ï¼ˆæ— éœ€æ•°æ®é›†ï¼‰

```bash
# æµ‹è¯• STFT å¤„ç†å™¨
cd cDDPM/V5
python -m signal.stft_utils

# æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
python -m models.uar_acssnet

# æµ‹è¯•æŸå¤±å‡½æ•°
python -m train.losses

# æµ‹è¯•æœ€å°è®­ç»ƒå¾ªç¯ï¼ˆéšæœºæ•°æ®ï¼‰
python -m train.min_train
```

**é¢„æœŸè¾“å‡º**:
- STFT é¢‘ç‡ bin éªŒè¯ï¼ˆ1-100 Hzï¼‰
- æ¨¡å‹å‚æ•°é‡ç»Ÿè®¡
- Shape å’ŒèŒƒå›´æ–­è¨€é€šè¿‡
- è®­ç»ƒ 2 epochï¼ˆæ¯ epoch 5 batchï¼‰å®Œæˆ

### 3. ä½¿ç”¨çœŸå®æ•°æ®é›†è®­ç»ƒ

```python
import sys
sys.path.append("cDDPM/V5")

from configs import get_default_config, print_config
from datasets import build_dataloaders
from models import UAR_ACSSNet
from train import train, set_seed
import torch
from pathlib import Path

# è®¾ç½®éšæœºç§å­
set_seed(42)

# åŠ è½½é…ç½®
cfg = get_default_config()
cfg["dataset_root"] = "../../Dataset"  # ä¿®æ”¹ä¸ºå®é™…è·¯å¾„
cfg["batch_size"] = 16
cfg["num_epochs"] = 50
print_config(cfg)

# æ„å»ºæ•°æ®åŠ è½½å™¨
loaders = build_dataloaders(
    root=cfg["dataset_root"],
    batch_size=cfg["batch_size"],
    segment_length=cfg["segment_length"],
    val_stride=cfg["val_stride"],
    test_stride=cfg["test_stride"],
    normalize=cfg["normalize"],
    num_workers=cfg["num_workers"],
    pin_memory=cfg["pin_memory"],
)

# åˆ›å»ºæ¨¡å‹
device = torch.device(cfg["device"] if torch.cuda.is_available() else "cpu")
model = UAR_ACSSNet(
    segment_length=cfg["segment_length"],
    unet_base_ch=cfg["unet_base_ch"],
    unet_levels=cfg["unet_levels"],
    spec_channels=cfg["spec_channels"],
    acss_depth=cfg["acss_depth"],
    num_freq_bins=cfg["num_freq_bins"],
    dropout=cfg["dropout"],
).to(device)

# ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=cfg["learning_rate"],
    weight_decay=cfg["weight_decay"],
)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=cfg["num_epochs"],
    eta_min=cfg["learning_rate"] * 0.01,
)

# è®­ç»ƒ
save_dir = Path(cfg["save_dir"])
save_dir.mkdir(exist_ok=True)

train(
    model=model,
    train_loader=loaders["train"],
    val_loader=loaders["val"],
    optimizer=optimizer,
    scheduler=scheduler,
    cfg=cfg,
    device=device,
    save_dir=save_dir,
)
```

---

## é…ç½®è¯´æ˜

### æ•°æ®é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `dataset_root` | str | `"../../Dataset"` | æ•°æ®é›†æ ¹ç›®å½• |
| `segment_length` | int | `2048` | è¾“å…¥ä¿¡å·é•¿åº¦ |
| `normalize` | str | `"zscore_per_sample"` | å½’ä¸€åŒ–æ–¹å¼ (`"none"` / `"zscore_per_sample"`) |
| `batch_size` | int | `16` | Batch size |
| `val_stride` | int | `1024` | éªŒè¯é›†æ»‘çª—æ­¥é•¿ |

### æ¨¡å‹é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `unet_base_ch` | int | `32` | U-Net åŸºç¡€é€šé“æ•° |
| `unet_levels` | int | `4` | U-Net ç¼–ç å™¨å±‚æ•° |
| `spec_channels` | int | `64` | è°±å›¾ç¼–ç å™¨è¾“å‡ºé€šé“æ•° |
| `acss_depth` | int | `3` | ACSSBlock å †å å±‚æ•° |
| `num_freq_bins` | int | `103` | STFT é¢‘ç‡ bin æ•°é‡ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰ |
| `dropout` | float | `0.1` | Dropout æ¯”ä¾‹ |

**å‚æ•°é‡ä¼°ç®—** (é»˜è®¤é…ç½®):
- U-Net: ~1.2M
- SpecEncoder + ACSS: ~0.5M
- FiLM + Confidence: ~0.3M
- **æ€»è®¡**: ~2M å‚æ•°

### æŸå¤±é…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `recon_weight` | float | `1.0` | é‡å»ºæŸå¤±æƒé‡ |
| `conf_reg_weight` | float | `0.1` | ç½®ä¿¡å›¾æ­£åˆ™æƒé‡ |
| `tv_weight` | float | `0.01` | TV å¹³æ»‘æ­£åˆ™ |
| `entropy_weight` | float | `0.01` | ç†µæ­£åˆ™ï¼ˆé˜²æ­¢é€€åŒ–ï¼‰ |
| `use_weighted_recon` | bool | `False` | æ˜¯å¦ä½¿ç”¨ç½®ä¿¡å›¾åŠ æƒé‡å»ºæŸå¤± |

**æŸå¤±å‡½æ•°å½¢å¼**:
```
L_total = Î»_recon * L_recon + Î»_conf_reg * L_conf_reg

L_recon = Charbonnier(y_hat, x_clean)  # å¯é€‰åŠ æƒ

L_conf_reg = Î»_tv * TV(w) + Î»_ent * Entropy(w)
  TV(w) = mean(|w[t+1] - w[t]|)
  Entropy(w) = -mean(w*log(w) + (1-w)*log(1-w))
```

### è®­ç»ƒé…ç½®

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `num_epochs` | int | `50` | è®­ç»ƒ epoch æ•° |
| `learning_rate` | float | `1e-4` | åˆå§‹å­¦ä¹ ç‡ |
| `weight_decay` | float | `1e-5` | æƒé‡è¡°å‡ |
| `grad_clip` | float | `1.0` | æ¢¯åº¦è£å‰ªé˜ˆå€¼ |
| `scheduler_type` | str | `"cosine"` | å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ |

---

## è®­ç»ƒä¸éªŒè¯

### è®­ç»ƒæ—¥å¿—ç¤ºä¾‹

```
============================================================
Starting training for 50 epochs
============================================================

[Epoch 1/50] Training...
  Epoch 1 [0/100] Loss: 0.3245 | Recon: 0.3123 | ConfReg: 0.0122
  Epoch 1 [10/100] Loss: 0.2987 | Recon: 0.2876 | ConfReg: 0.0111
  ...

[Epoch 1/50] Validating...

[Epoch 1/50] Summary:
  Time: 45.23s
  Train Loss: 0.287654 | Recon: 0.276432
  Val   Loss: 0.254321 | Recon: 0.243210
  LR: 0.000100
  âœ“ Saved best model to checkpoints/best_model.pth

...
```

### æ¨ç†ä½¿ç”¨

```python
import torch
from models import UAR_ACSSNet

# åŠ è½½æ¨¡å‹
checkpoint = torch.load("checkpoints/best_model.pth")
model = UAR_ACSSNet(**checkpoint['cfg'])
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# æ¨ç†
x_raw = torch.randn(1, 1, 2048)  # (B, 1, L)
with torch.no_grad():
    outputs = model(x_raw)

y_hat = outputs["y_hat"]  # é‡å»ºä¿¡å· (1, 1, 2048)
w = outputs["w"]          # ç½®ä¿¡å›¾ (1, 1, 2048)
```

---

## æŠ€æœ¯ç»†èŠ‚

### STFT å‚æ•°ï¼ˆå›ºå®šï¼Œä¸å¯æ›´æ”¹ï¼‰

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|-----|------|
| `fs` | 500 Hz | é‡‡æ ·ç‡ |
| `n_fft` | 512 | FFT ç‚¹æ•° |
| `hop_length` | 64 | å¸§ç§» |
| `win_length` | 156 | çª—é•¿ |
| `window` | Hann | çª—å‡½æ•° |
| é¢‘ç‡åˆ†è¾¨ç‡ | ~0.977 Hz | `fs / n_fft` |
| é¢‘ç‡èŒƒå›´ | 1â€“100 Hz | é€‰æ‹© bin [1, 102]ï¼ˆå…± 103 binsï¼‰ |

**é¢‘ç‡ bin è®¡ç®—**:
```python
df = 500 / 512 â‰ˆ 0.977 Hz
k_min = ceil(1.0 / df) = 2
k_max = floor(100.0 / df) = 102
num_bins = 102 - 2 + 1 = 103
```

### Shape æµç¨‹è¿½è¸ª

å‡è®¾è¾“å…¥ `x_raw (4, 1, 2048)`:

```
1. STFT:
   (4, 1, 2048) â†’ stft â†’ (4, 257, T)  [T â‰ˆ 35]
   â†’ select bins [2:103] â†’ (4, 103, 35)

2. SpecEncoder2D:
   (4, 103, 35) â†’ (4, 64, 35, 103)  [permute to (B,C,T,F)]

3. ACSSStack (depth=3):
   (4, 64, 35, 103) â†’ ACSSBlock Ã— 3 â†’ (4, 64, 35, 103)

4. FiLM:
   (4, 64, 35, 103) â†’ pool F â†’ (4, 64, 35)
   â†’ interpolate to L â†’ (4, 64, 2048)
   â†’ generate Î±, Î² for decoder layers

5. U-Net:
   (4, 1, 2048) + FiLM â†’ (4, 1, 2048)

6. Confidence:
   (4, 64, 35, 103) â†’ pool & head â†’ (4, 1, 35)
   â†’ interpolate to L â†’ (4, 1, 2048)
   â†’ sigmoid â†’ [0, 1]
```

### å†…å­˜ä¸é€Ÿåº¦ä¼°ç®—

**å•æœºå•å¡ (RTX 3090 24GB)**:
- Batch size 16, L=2048: ~4GB
- è®­ç»ƒé€Ÿåº¦: ~150 samples/s
- å• epoch (10k samples): ~70s

---

## å¯å¤ç°æ€§

### éšæœºç§å­è®¾ç½®

æ‰€æœ‰éšæœºæ€§å‡é€šè¿‡ `set_seed(42)` å›ºå®šï¼š
```python
from train import set_seed
set_seed(42)
```

åŒ…å«:
- Python `random`
- NumPy `np.random`
- PyTorch `torch.manual_seed`
- CUDA `torch.cuda.manual_seed_all`
- cuDNN `deterministic=True, benchmark=False`

### Dtype ä¸è®¾å¤‡

- **Dtype**: æ‰€æœ‰è®¡ç®—ä½¿ç”¨ `float32`
- **è®¾å¤‡**: è‡ªåŠ¨æ£€æµ‹ CUDA æˆ– CPU
- **AMP**: æœªå¯ç”¨ï¼ˆå¯è‡ªè¡Œæ·»åŠ ï¼‰

### æ–­è¨€æ£€æŸ¥

ä»£ç ä¸­åŒ…å«å¤§é‡ shape å’ŒèŒƒå›´æ–­è¨€ï¼š
```python
assert y_hat.shape == (B, 1, L)
assert (w >= 0).all() and (w <= 1).all()
assert S.shape[1] == num_freq_bins
```

è¿è¡Œæ—¶ä¼šè‡ªåŠ¨éªŒè¯ï¼Œç¡®ä¿æ•°æ®æµæ­£ç¡®ã€‚

---

## æ‰©å±•æŒ‡å—

### 1. æ›¿æ¢ä¸ºçœŸå® Mamba/SSM

å½“å‰ `ScanFreq/ScanTime` ä½¿ç”¨ç®€åŒ–å®ç°ï¼ˆdepthwise convï¼‰ã€‚æ›¿æ¢æ­¥éª¤ï¼š

```python
# åœ¨ models/uar_acssnet.py ä¸­

# åŸå®ç°
class ScanFreq(nn.Module):
    def __init__(self, channels: int):
        super().__init__()
        self.scan = DepthwiseScan1D(channels, kernel_size=5)
    ...

# æ›¿æ¢ä¸º Mamba
from mamba_ssm import Mamba

class ScanFreq(nn.Module):
    def __init__(self, channels: int):
        super().__init__()
        self.mamba = Mamba(d_model=channels, d_state=16, d_conv=4)
    
    def forward(self, x):
        # x: (B, C, T, F)
        B, C, T, F = x.shape
        x_reshaped = x.permute(0, 2, 1, 3).reshape(B*T, C, F)
        # Mamba expects (B, L, D)
        x_in = x_reshaped.permute(0, 2, 1)  # (B*T, F, C)
        out = self.mamba(x_in)  # (B*T, F, C)
        out = out.permute(0, 2, 1).reshape(B, T, C, F).permute(0, 2, 1, 3)
        return out
```

### 2. æ·»åŠ æ•°æ®å¢å¼º

åœ¨ `datasets/eeg_pair_dataset.py` ä¸­æ·»åŠ å¢å¼ºï¼š

```python
def augment(self, x: np.ndarray) -> np.ndarray:
    # æ—¶é—´å¹³ç§»
    shift = np.random.randint(-100, 100)
    x = np.roll(x, shift, axis=-1)
    
    # å¹…å€¼ç¼©æ”¾
    scale = np.random.uniform(0.9, 1.1)
    x = x * scale
    
    return x
```

### 3. å¤š GPU è®­ç»ƒ

```python
from torch.nn.parallel import DataParallel

model = UAR_ACSSNet(...)
if torch.cuda.device_count() > 1:
    model = DataParallel(model)
model.to(device)
```

---

## æ¨ç†ä¸éƒ¨ç½²

### æ–‡ä»¶çº§æ¨ç†

è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ `inference_file.py` å¯¹æ–°æ•°æ®è¿›è¡Œé™å™ªï¼š

#### å•æ–‡ä»¶æ¨ç†

```bash
python inference_file.py \
    --checkpoint output_V5/checkpoints/best_model.pth \
    --input data/noisy_signal.npy \
    --output results/denoised_signal.npy \
    --segment_length 2048 \
    --stride 1024
```

#### æ‰¹é‡æ¨ç†ï¼ˆç›®å½•ï¼‰

```bash
python inference_file.py \
    --checkpoint output_V5/checkpoints/best_model.pth \
    --input data/raw_signals/ \
    --output results/denoised/ \
    --pattern "*.npy" \
    --batch_size 32
```

#### æ¨ç†å‚æ•°è¯´æ˜

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--checkpoint` | - | æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰|
| `--input` | - | è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•ï¼ˆå¿…éœ€ï¼‰|
| `--output` | - | è¾“å‡ºæ–‡ä»¶æˆ–ç›®å½•ï¼ˆå¿…éœ€ï¼‰|
| `--segment_length` | 2048 | åˆ†å‰²ç‰‡æ®µé•¿åº¦ |
| `--stride` | 1024 | æ»‘çª—æ­¥é•¿ï¼ˆå»ºè®®ä¸ºsegment_length/2ï¼‰|
| `--normalize` | zscore | å½’ä¸€åŒ–æ–¹æ³•ï¼ˆzscore/minmax/noneï¼‰|
| `--batch_size` | 32 | æ‰¹å¤„ç†å¤§å° |
| `--save_format` | npy | ä¿å­˜æ ¼å¼ï¼ˆnpy/npz/txtï¼‰|
| `--device` | cuda | è®¾å¤‡ï¼ˆcuda/cpuï¼‰|

**é•¿ä¿¡å·å¤„ç†**ï¼šè‡ªåŠ¨ä½¿ç”¨æ»‘çª—åˆ†å‰² â†’ æ‰¹é‡æ¨ç† â†’ é‡å å¹³å‡é‡å»ºï¼Œä¿æŒä¿¡å·å®Œæ•´æ€§ã€‚

### ç»“æœå¯è§†åŒ–

ä½¿ç”¨ `visualize_inference.py` æ¯”è¾ƒåŸå§‹å’Œé™å™ªä¿¡å·ï¼š

```bash
python visualize_inference.py \
    --raw data/test_001_raw.npy \
    --denoised results/test_001_denoised.npy \
    --clean data/test_001_clean.npy \
    --spectral \
    --save comparison.png
```

### å¿«é€Ÿæµ‹è¯•

è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯æ¨ç†åŠŸèƒ½ï¼š

```bash
python test_inference.py
```

è¯¥è„šæœ¬ä¼šï¼š
1. æ£€æŸ¥æ¨¡å‹å’Œæ•°æ®æ–‡ä»¶
2. æ‰§è¡Œå•æ–‡ä»¶æ¨ç†æµ‹è¯•
3. æ‰§è¡Œæ‰¹é‡æ¨ç†æµ‹è¯•
4. éªŒè¯è¾“å‡ºæ–‡ä»¶å¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯

**è¯¦ç»†æ–‡æ¡£**: æŸ¥çœ‹ [`INFERENCE_README.md`](INFERENCE_README.md) è·å–å®Œæ•´æ¨ç†æŒ‡å—ã€‚

---

## å¸¸è§é—®é¢˜

### Q1: STFT é¢‘ç‡ bin ä¸åŒ¹é…ï¼Ÿ

**A**: æ£€æŸ¥ `num_freq_bins` æ˜¯å¦ä¸ STFT é…ç½®ä¸€è‡´ã€‚é»˜è®¤ `fs=500, n_fft=512, freq_range=[1,100]` å¯¹åº” **103 bins**ã€‚

### Q2: å†…å­˜æº¢å‡ºï¼Ÿ

**A**: å‡å° `batch_size` æˆ– `segment_length`ã€‚æ¨èé…ç½®ï¼š
- 8GB GPU: batch_size=8, L=2048
- 16GB GPU: batch_size=16, L=2048
- 24GB GPU: batch_size=32, L=2048

### Q3: ç½®ä¿¡å›¾ `w` å…¨ä¸º 0.5ï¼Ÿ

**A**: å¯èƒ½æ˜¯æ­£åˆ™æƒé‡è¿‡å¤§å¯¼è‡´é€€åŒ–ã€‚å°è¯•ï¼š
- é™ä½ `entropy_weight` (0.01 â†’ 0.001)
- å¢åŠ  `conf_reg_weight` çš„è®­ç»ƒ epoch å»¶è¿Ÿ

### Q4: éªŒè¯æŸå¤±ä¸ä¸‹é™ï¼Ÿ

**A**: æ£€æŸ¥ï¼š
1. æ•°æ®é›†æ˜¯å¦æ­£ç¡®é…å¯¹ï¼ˆraw/clean æ–‡ä»¶åä¸€è‡´ï¼‰
2. å½’ä¸€åŒ–æ˜¯å¦åˆç†ï¼ˆå»ºè®®ä½¿ç”¨ `zscore_per_sample`ï¼‰
3. å­¦ä¹ ç‡æ˜¯å¦è¿‡å¤§ï¼ˆé™ä½åˆ° `1e-5` è¯•è¯•ï¼‰

### Q5: æ¨ç†æ—¶æ˜¾å­˜ä¸è¶³ï¼Ÿ

**A**: å‡å°æ¨ç†æ‰¹å¤„ç†å¤§å°æˆ–åˆ†å‰²é•¿åº¦ï¼š
```bash
--batch_size 8 --segment_length 1024
```

---

## å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ï¼ˆç¤ºä¾‹ï¼‰ï¼š

```bibtex
@misc{uar_acssnet2026,
  title={UAR-ACSSNet: Unified Artifact Removal with Axis-Conditioned Selective Scan},
  author={Your Name},
  year={2026},
  howpublished={\url{https://github.com/yourrepo/uar-acssnet}}
}
```

---

## è®¸å¯è¯

MIT License

---

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ–è”ç³»ï¼š`your.email@example.com`

---

**Happy Training! ğŸš€**
