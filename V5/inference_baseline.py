"""
Baselineæ¨¡å‹å•æ ·æœ¬æ¨ç†è„šæœ¬
ä»éªŒè¯é›†åŠ è½½æ ·æœ¬ï¼Œä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†å¹¶å¯è§†åŒ–ç»“æœ
"""
import sys
from pathlib import Path
import torch
import numpy as np
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from models import UAR_ACSSNet
from datasets import build_dataloaders
from configs import get_default_config


def load_model(checkpoint_path, device='cuda'):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡
    
    Returns:
        model: åŠ è½½å¥½çš„æ¨¡å‹
        cfg: é…ç½®å­—å…¸
    """
    print(f"\nğŸ“¦ Loading checkpoint from: {checkpoint_path}")
    ckpt = torch.load(checkpoint_path, map_location=device)
    
    # ä»checkpointè·å–é…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰æˆ–ä½¿ç”¨é»˜è®¤é…ç½®
    if 'cfg' in ckpt:
        cfg = ckpt['cfg']
        print("âœ“ Using config from checkpoint")
    else:
        cfg = get_default_config()
        print("âœ“ Using default config")
    
    # åˆ›å»ºæ¨¡å‹
    model = UAR_ACSSNet(
        segment_length=cfg.get("segment_length", 2048),
        unet_base_ch=cfg.get("unet_base_ch", 32),
        unet_levels=cfg.get("unet_levels", 4),
        spec_channels=cfg.get("spec_channels", 64),
        acss_depth=cfg.get("acss_depth", 3),
        num_freq_bins=cfg.get("num_freq_bins", 101),
        dropout=cfg.get("dropout", 0.0),
        baseline_mode=cfg.get("baseline_mode", True),
    ).to(device)
    
    # åŠ è½½æƒé‡
    model.load_state_dict(ckpt['model_state_dict'])
    model.eval()
    
    print(f"âœ“ Loaded model from epoch {ckpt.get('epoch', 'unknown')}")
    print(f"âœ“ Best val loss: {ckpt.get('val_loss', 'unknown'):.6f}")
    
    total_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ Total parameters: {total_params:,}")
    
    return model, cfg


def inference_and_visualize(
    model,
    val_loader,
    device='cuda',
    num_samples=4,
    save_path="inference_results.png"
):
    """
    æ¨ç†å¹¶å¯è§†åŒ–ç»“æœ
    
    Args:
        model: æ¨¡å‹
        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        num_samples: å¯è§†åŒ–æ ·æœ¬æ•°é‡
        save_path: ä¿å­˜è·¯å¾„
    """
    print(f"\nğŸ” Running inference on {num_samples} samples...")
    
    # è·å–ä¸€ä¸ªbatch
    batch = next(iter(val_loader))
    
    # è§£æbatchï¼ˆæ ¹æ®æ•°æ®é›†è¿”å›æ ¼å¼ï¼‰
    if isinstance(batch, (list, tuple)):
        if len(batch) == 2:
            x_raw, x_clean = batch
        else:
            x_raw, x_clean, _ = batch
    else:
        x_raw = batch['raw']
        x_clean = batch['clean']
    
    x_raw = x_raw.to(device)
    x_clean = x_clean.to(device)
    
    # æ¨ç†
    with torch.no_grad():
        outputs = model(x_raw)
        y_hat = outputs['y_hat']
        w = outputs.get('w', None)
    
    # è®¡ç®—MSE
    mse = torch.mean((y_hat - x_clean) ** 2, dim=-1).cpu().numpy()
    
    print(f"âœ“ Inference completed")
    print(f"  - Input shape: {x_raw.shape}")
    print(f"  - Output shape: {y_hat.shape}")
    print(f"  - MSE range: [{mse.min():.6f}, {mse.max():.6f}]")
    print(f"  - MSE mean: {mse.mean():.6f}")
    
    # å¯è§†åŒ–
    num_samples = min(num_samples, x_raw.shape[0])
    fig, axes = plt.subplots(num_samples, 1, figsize=(15, 3 * num_samples))
    
    if num_samples == 1:
        axes = [axes]
    
    for i in range(num_samples):
        ax = axes[i]
        
        # è½¬æ¢ä¸ºnumpy
        raw_np = x_raw[i, 0].cpu().numpy()
        clean_np = x_clean[i, 0].cpu().numpy()
        pred_np = y_hat[i, 0].cpu().numpy()
        
        # ç»˜åˆ¶
        time_axis = np.arange(len(raw_np))
        ax.plot(time_axis, raw_np, 'k', alpha=0.4, linewidth=1, label='Raw (Noisy)')
        ax.plot(time_axis, clean_np, 'g', linewidth=1.5, label='Ground Truth (Clean)')
        ax.plot(time_axis, pred_np, 'r--', linewidth=1.5, label='Prediction (Denoised)')
        
        # æ ‡é¢˜å’Œæ ‡ç­¾
        sample_mse = mse[i, 0]
        ax.set_title(f"Sample {i+1} - MSE: {sample_mse:.6f}", fontsize=12, fontweight='bold')
        ax.set_xlabel('Time (samples)', fontsize=10)
        ax.set_ylabel('Amplitude', fontsize=10)
        ax.legend(loc='upper right', fontsize=9)
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        info_text = (
            f"Raw: Î¼={raw_np.mean():.3f}, Ïƒ={raw_np.std():.3f}\n"
            f"Clean: Î¼={clean_np.mean():.3f}, Ïƒ={clean_np.std():.3f}\n"
            f"Pred: Î¼={pred_np.mean():.3f}, Ïƒ={pred_np.std():.3f}"
        )
        ax.text(0.02, 0.98, info_text, transform=ax.transAxes,
                verticalalignment='top', fontsize=8,
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… Visualization saved to: {save_path}")
    
    # å¦‚æœæœ‰confidence mapï¼Œé¢å¤–ä¿å­˜
    if w is not None and not model.baseline_mode:
        save_confidence_map(w, x_raw, num_samples, save_path.replace('.png', '_confidence.png'))


def save_confidence_map(w, x_raw, num_samples, save_path):
    """
    ä¿å­˜confidence mapå¯è§†åŒ–
    
    Args:
        w: confidence map (B, 1, L)
        x_raw: è¾“å…¥ä¿¡å·
        num_samples: æ ·æœ¬æ•°é‡
        save_path: ä¿å­˜è·¯å¾„
    """
    num_samples = min(num_samples, w.shape[0])
    fig, axes = plt.subplots(num_samples, 1, figsize=(15, 2 * num_samples))
    
    if num_samples == 1:
        axes = [axes]
    
    for i in range(num_samples):
        ax = axes[i]
        
        w_np = w[i, 0].cpu().numpy()
        raw_np = x_raw[i, 0].cpu().numpy()
        time_axis = np.arange(len(w_np))
        
        # åŒyè½´ï¼šä¿¡å· + confidence
        ax2 = ax.twinx()
        ax.plot(time_axis, raw_np, 'k', alpha=0.3, linewidth=0.5, label='Raw Signal')
        ax2.plot(time_axis, w_np, 'b-', linewidth=1.5, label='Confidence Map')
        ax2.fill_between(time_axis, 0, w_np, alpha=0.3, color='blue')
        
        ax.set_xlabel('Time (samples)')
        ax.set_ylabel('Amplitude', color='k')
        ax2.set_ylabel('Confidence w(t)', color='b')
        ax2.set_ylim([0, 1])
        
        ax.set_title(f"Sample {i+1} - Confidence Map (mean={w_np.mean():.3f}, std={w_np.std():.3f})")
        ax.legend(loc='upper left')
        ax2.legend(loc='upper right')
        ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    print(f"âœ… Confidence map saved to: {save_path}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Inference with trained baseline model')
    parser.add_argument('--checkpoint', type=str, default='output_V5/checkpoints/best_model.pth',
                        help='Path to checkpoint file')
    parser.add_argument('--dataset_root', type=str, default='../../Dataset',
                        help='Dataset root directory')
    parser.add_argument('--num_samples', type=int, default=4,
                        help='Number of samples to visualize')
    parser.add_argument('--output', type=str, default='inference_baseline_vis.png',
                        help='Output visualization file path')
    parser.add_argument('--device', type=str, default='cuda',
                        help='Device to use (cuda/cpu)')
    args = parser.parse_args()
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # åŠ è½½æ¨¡å‹
    checkpoint_path = Path(args.checkpoint)
    if not checkpoint_path.exists():
        print(f"âŒ Checkpoint not found: {checkpoint_path}")
        print(f"   Please train the model first or specify correct checkpoint path")
        return
    
    model, cfg = load_model(checkpoint_path, device)
    
    # æ›´æ–°æ•°æ®é›†è·¯å¾„
    cfg['dataset_root'] = args.dataset_root
    cfg['batch_size'] = max(4, args.num_samples)  # è‡³å°‘åŠ è½½è¶³å¤Ÿçš„æ ·æœ¬
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    print(f"\nğŸ“Š Loading validation dataset from: {cfg['dataset_root']}")
    _, val_loader, _ = build_dataloaders(
        dataset_root=cfg['dataset_root'],
        batch_size=cfg['batch_size'],
        segment_length=cfg['segment_length'],
        train_stride=cfg.get('train_stride'),
        val_stride=cfg.get('val_stride', 1024),
        test_stride=cfg.get('test_stride', 1024),
        normalize=cfg['normalize'],
        num_workers=0,  # å•çº¿ç¨‹é¿å…æ½œåœ¨é—®é¢˜
        pin_memory=False,
        return_meta=False,
    )
    
    print(f"âœ“ Validation loader ready (batch_size={cfg['batch_size']})")
    
    # æ¨ç†å¹¶å¯è§†åŒ–
    inference_and_visualize(
        model=model,
        val_loader=val_loader,
        device=device,
        num_samples=args.num_samples,
        save_path=args.output
    )
    
    print(f"\nğŸ‰ Inference completed successfully!")
    print(f"   Results saved to: {args.output}")


if __name__ == "__main__":
    main()
