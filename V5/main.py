"""
å®Œæ•´è®­ç»ƒè„šæœ¬ - ä½¿ç”¨çœŸå®æ•°æ®é›†
"""
import sys
import argparse
from pathlib import Path
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from configs import get_default_config, print_config
from datasets import build_dataloaders
from models import UAR_ACSSNet
from train import train, set_seed


def parse_args():
    parser = argparse.ArgumentParser(description="Train UAR-ACSSNet")
    
    # æ•°æ®é…ç½®
    # é»˜è®¤è·¯å¾„ï¼šç›¸å¯¹äºè„šæœ¬æ–‡ä»¶å‘ä¸Šä¸¤çº§åˆ° 3_ICAï¼Œå†åˆ° Dataset
    default_dataset = str(Path(__file__).parent.parent / "Dataset")
    parser.add_argument("--dataset_root", type=str, default=default_dataset,
                        help="æ•°æ®é›†æ ¹ç›®å½•")
    parser.add_argument("--batch_size", type=int, default=16,
                        help="Batch size")
    parser.add_argument("--segment_length", type=int, default=2048,
                        help="è¾“å…¥ä¿¡å·é•¿åº¦")
    parser.add_argument("--num_workers", type=int, default=4,
                        help="DataLoader workers")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--unet_base_ch", type=int, default=32,
                        help="U-Net åŸºç¡€é€šé“æ•°")
    parser.add_argument("--unet_levels", type=int, default=4,
                        help="U-Net ç¼–ç å™¨å±‚æ•°")
    parser.add_argument("--spec_channels", type=int, default=64,
                        help="è°±å›¾ç¼–ç å™¨è¾“å‡ºé€šé“æ•°")
    parser.add_argument("--acss_depth", type=int, default=3,
                        help="ACSSBlock å †å å±‚æ•°")
    parser.add_argument("--dropout", type=float, default=0.1,
                        help="Dropout æ¯”ä¾‹")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--num_epochs", type=int, default=200,
                        help="è®­ç»ƒ epoch æ•°")
    parser.add_argument("--learning_rate", type=float, default=1e-4,
                        help="åˆå§‹å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-5,
                        help="æƒé‡è¡°å‡")
    parser.add_argument("--grad_clip", type=float, default=1.0,
                        help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    
    # å…¶ä»–
    parser.add_argument("--save_dir", type=str, default="output_V5/checkpoints",
                        help="æ¨¡å‹ä¿å­˜ç›®å½•")
    parser.add_argument("--seed", type=int, default=42,
                        help="éšæœºç§å­")
    parser.add_argument("--device", type=str, default="cuda",
                        help="è®¾å¤‡ (cuda/cpu)")
    
    # å¿«é€Ÿæµ‹è¯•
    parser.add_argument("--quick_test", action="store_true",
                        help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼ˆå°‘é‡ epoch å’Œ batchï¼‰")
    
    return parser.parse_args()


def main():
    # è§£æå‚æ•°
    args = parse_args()
    
    # åŠ è½½é»˜è®¤é…ç½®
    cfg = get_default_config()
    
    # æ›´æ–°é…ç½®
    for key, value in vars(args).items():
        if key in cfg:
            cfg[key] = value
    
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    if args.quick_test:
        print("\nâš¡ Quick Test Mode Enabled")
        cfg["num_epochs"] = 2
        cfg["max_train_batches"] = 5
        cfg["max_val_batches"] = 3
        cfg["log_interval"] = 1
    
    # æ‰“å°é…ç½®
    print_config(cfg)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(cfg["seed"])
    
    # è®¾å¤‡
    device = torch.device(cfg["device"] if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}\n")
    
    # æ„å»ºæ•°æ®åŠ è½½å™¨
    print("Loading dataset...")
    loaders = build_dataloaders(
        root=cfg["dataset_root"],
        batch_size=cfg["batch_size"],
        segment_length=cfg["segment_length"],
        train_stride=cfg.get("train_stride"),
        val_stride=cfg.get("val_stride", 1024),
        test_stride=cfg.get("test_stride", 1024),
        normalize=cfg["normalize"],
        num_workers=cfg["num_workers"],
        pin_memory=cfg["pin_memory"],
        return_meta=False,
    )
    
    # åˆ›å»ºæ¨¡å‹
    print("\nBuilding model...")
    model = UAR_ACSSNet(
        segment_length=cfg["segment_length"],
        unet_base_ch=cfg["unet_base_ch"],
        unet_levels=cfg["unet_levels"],
        spec_channels=cfg["spec_channels"],
        acss_depth=cfg["acss_depth"],
        num_freq_bins=cfg["num_freq_bins"],
        dropout=cfg["dropout"],
        baseline_mode=cfg.get("baseline_mode", False),
    ).to(device)
    
    if cfg.get("baseline_mode", False):
        print("ğŸ”¹ Baseline Mode: Using pure U-Net (no FiLM/ACSS)")
    else:
        print("ğŸ”¹ Full Mode: UAR-ACSSNet with FiLM modulation")
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Model parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}\n")
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=cfg["learning_rate"],
        weight_decay=cfg["weight_decay"],
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = None
    if cfg.get("use_scheduler", True):
        if cfg.get("scheduler_type", "cosine") == "cosine":
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=cfg["num_epochs"],
                eta_min=cfg["learning_rate"] * 0.01,
            )
        elif cfg["scheduler_type"] == "step":
            scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer,
                step_size=cfg["num_epochs"] // 3,
                gamma=0.1,
            )
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(cfg["save_dir"])
    save_dir.mkdir(exist_ok=True, parents=True)
    
    # è®­ç»ƒ
    train(
        model=model,
        train_loader=loaders["train"],
        val_loader=loaders["val"],
        optimizer=optimizer,
        scheduler=scheduler,
        cfg=cfg,
        device=device,
        save_dir=save_dir,
    )
    
    print("\nâœ“ Training completed!")
    print(f"âœ“ Best model saved to: {save_dir / 'best_model.pth'}\n")


if __name__ == "__main__":
    main()
